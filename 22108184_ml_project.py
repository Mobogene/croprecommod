# -*- coding: utf-8 -*-
"""22108184 ML Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qjKjAurE6eX9glp-oolAEqGosn7M48yd
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Load the dataset
data = pd.read_csv('/content/Crop_recommendation.csv')

data.head()

data.isna().sum()

# Separate features (X) and target variable (y)
X = data.drop('label', axis=1)  # Replace 'target_variable_column'
y = data['label']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the Random Forest Classifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42) # You can adjust n_estimators
rf_classifier.fit(X_train, y_train)

# Make predictions on the test set
y_pred = rf_classifier.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Load the dataset
data = pd.read_csv('/content/Crop_recommendation.csv')

# Separate features (X) and target variable (y)
X = data.drop('label', axis=1)  # Replace 'target_variable_column'
y = data['label']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the SVM classifier
svm_classifier = SVC(kernel='linear', random_state=42) # You can change the kernel (e.g., 'rbf', 'poly')
svm_classifier.fit(X_train, y_train)

# Make predictions on the test set
y_pred = svm_classifier.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
data = pd.read_csv('/content/Crop_recommendation.csv')

# Select only numerical features for correlation analysis
numerical_features = data.select_dtypes(include=['number'])

# Calculate the correlation matrix
correlation_matrix = numerical_features.corr()

# Create the heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix of Crop Recommendation Dataset (Numerical Features)')
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Load the dataset
data = pd.read_csv('/content/Crop_recommendation.csv')

# Separate features (X) and target variable (y)
X = data.drop('label', axis=1)
y = data['label']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the Logistic Regression model
logreg_classifier = LogisticRegression(max_iter=1000, random_state=42) # Increased max_iter
logreg_classifier.fit(X_train, y_train)

# Make predictions on the test set
y_pred = logreg_classifier.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")

import matplotlib.pyplot as plt

# Accuracy scores
model_names = ['Random Forest', 'SVM', 'Logistic Regression']
accuracies = [0.993, 0.97, 0.952]

# Create the bar plot
plt.figure(figsize=(8, 6))
plt.bar(model_names, accuracies, color=['skyblue', 'lightcoral', 'lightgreen'])
plt.xlabel("Models")
plt.ylabel("Accuracy")
plt.title("Model Accuracies Comparison")
plt.ylim(0, 1)  # Set y-axis limit to 0-1 for accuracy
plt.grid(axis='y', linestyle='--', alpha=0.7)


# Add accuracy values on top of each bar
for i, v in enumerate(accuracies):
    plt.text(i, v + 0.01, f"{v:.2f}", ha='center', va='bottom')

plt.show()

!pip install gradio

import gradio as gr
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# Load the dataset (replace with your actual path)
data = pd.read_csv('/content/Crop_recommendation.csv')

# Prepare the data (same as your existing code)
X = data.drop('label', axis=1)
y = data['label']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the RandomForestClassifier (you can choose another model here if preferred)
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
rf_classifier.fit(X_train, y_train)

def predict_crop(N, P, K, temperature, humidity, ph, rainfall):
    """Predicts the crop based on input features."""
    input_data = pd.DataFrame([[N, P, K, temperature, humidity, ph, rainfall]],
                              columns=['N', 'P', 'K', 'temperature', 'humidity', 'ph', 'rainfall'])
    prediction = rf_classifier.predict(input_data)[0]
    return prediction

# Create the Gradio interface
iface = gr.Interface(
    fn=predict_crop,
    inputs=[
        gr.Number(label="Nitrogen (N)"),
        gr.Number(label="Phosphorus (P)"),
        gr.Number(label="Potassium (K)"),
        gr.Number(label="Temperature (°C)"),
        gr.Number(label="Humidity (%)"),
        gr.Number(label="pH"),
        gr.Number(label="Rainfall (mm)"),
    ],
    outputs=gr.Textbox(label="Predicted Crop"),
    title="Crop Recommendation",
    description="Enter the features to predict the best crop."
)

# Launch the interface
iface.launch()

# prompt: make this streamlib to save prediction history and show it just like chatgbt

import pandas as pd
import gradio as gr

# ... (Your existing code for data preprocessing, model training, etc.)

# Initialize an empty list to store prediction history
prediction_history = []

def predict_crop(N, P, K, temperature, humidity, ph, rainfall):
    """Predicts the crop based on input features and saves the prediction."""
    input_data = pd.DataFrame([[N, P, K, temperature, humidity, ph, rainfall]],
                              columns=['N', 'P', 'K', 'temperature', 'humidity', 'ph', 'rainfall'])
    prediction = rf_classifier.predict(input_data)[0]

    # Append the input features and prediction to the history
    prediction_history.append({
        'N': N, 'P': P, 'K': K, 'temperature': temperature,
        'humidity': humidity, 'ph': ph, 'rainfall': rainfall,
        'predicted_crop': prediction
    })

    # Create a formatted string for the chat-like output
    history_str = ""
    for entry in prediction_history:
        history_str += f"**Input:** N={entry['N']}, P={entry['P']}, K={entry['K']}, Temp={entry['temperature']}, Humidity={entry['humidity']}, pH={entry['ph']}, Rainfall={entry['rainfall']}\n"
        history_str += f"**Prediction:** {entry['predicted_crop']}\n\n"

    return history_str  # Return the formatted history

# Create the Gradio interface
iface = gr.Interface(
    fn=predict_crop,
    inputs=[
        gr.Number(label="Nitrogen (N)"),
        gr.Number(label="Phosphorus (P)"),
        gr.Number(label="Potassium (K)"),
        gr.Number(label="Temperature (°C)"),
        gr.Number(label="Humidity (%)"),
        gr.Number(label="pH"),
        gr.Number(label="Rainfall (mm)"),
    ],
    outputs=gr.Markdown(label="Prediction History"), # Use Markdown output
    title="Crop Recommendation",
    description="Enter the features to predict the best crop."
)

# Launch the interface
iface.launch()